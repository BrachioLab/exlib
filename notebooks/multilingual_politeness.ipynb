{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "from transformers import Pipeline\n",
    "from torch import Tensor \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "import sentence_transformers\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_REPO = \"shreyahavaldar/multilingual_politeness\"\n",
    "MODEL_REPO = \"shreyahavaldar/xlm-roberta-politeness\"\n",
    "TOKENIZER_REPO = \"xlm-roberta-base\"\n",
    "\n",
    "def load_data():\n",
    "    hf_dataset = load_dataset(DATASET_REPO)\n",
    "    return hf_dataset\n",
    "\n",
    "def load_model():\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(MODEL_REPO)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "class PolitenessDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split, language=\"english\"):\n",
    "        dataset = load_dataset(DATASET_REPO)[split]\n",
    "        dataset = dataset.filter(lambda x: x[\"language\"] == language)\n",
    "        dataset = dataset.rename_column(\"politeness\", \"label\")\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(TOKENIZER_REPO)\n",
    "        self.max_len = max([len(text.split()) for text in dataset['Utterance']])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[\"Utterance\"][idx]\n",
    "        label = self.dataset[\"label\"][idx]\n",
    "        encoding = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "        word_list = text.split()\n",
    "        for i in range(len(word_list), self.max_len):\n",
    "            word_list.append('')\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"label\": torch.tensor(label),\n",
    "            'word_list': word_list\n",
    "        }\n",
    "\n",
    "class PolitenessClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolitenessClassifier, self).__init__()\n",
    "        self.model = load_model()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        logits = outputs.logits\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample inference on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: That is why he is a ‘pretender’. He has never claimed to be a King - or a Kaiser, for that matter. He is in the same class as the Comte de Paris, who is not the King of France, but would be if the Bourbons were placed on a restored French throne.\n",
      "Politeness: -0.11096464097499847\n",
      "\n",
      "Text: Let's knock any 'EngVar' shenanigans on the head right away, shall we? The Manual of Style, as I understand it, makes it clear that the subject's national ties and own language set the course.\n",
      "Politeness: -0.4824700355529785\n",
      "\n",
      "Text: Thank you for your contributions. There are some conventions that apply to articles, and medical articles in particular. Secondary sources were available for the material, and should be cited to validate the medical information from the studies.\n",
      "Politeness: 1.5924450159072876\n",
      "\n",
      "Text: The conversion of tacit to explicit knowledge is seen in for example the bread making machine's case. In response to your question, culture is a broad term. I would like to narrow it down to organization culture.\n",
      "Politeness: 0.7522311210632324\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = PolitenessDataset(\"train\")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "model = PolitenessClassifier()\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(dataloader): \n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    utterances = [dataset.tokenizer.decode(input_id, skip_special_tokens=True) for input_id in input_ids]\n",
    "    for utterance, label in zip(utterances, output):\n",
    "        print(\"Text: {}\\nPoliteness: {}\\n\".format(utterance, label.item()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Alignment Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(nn.Module): \n",
    "    def __init__(self, model_name:str=\"distiluse-base-multilingual-cased\"): \n",
    "        super(Metric, self).__init__()\n",
    "        self.model = sentence_transformers.SentenceTransformer(model_name)\n",
    "        self.centroids = self.get_centroids()\n",
    "    \n",
    "    def get_centroids(self):\n",
    "        # read lexica files\n",
    "        languages = [\"english\", \"spanish\", \"chinese\", \"japanese\"]\n",
    "        lexica = {}\n",
    "        for l in languages:\n",
    "            filepath = f\"../src/exlib/utils/politeness_lexica/{l}_politelex.csv\"\n",
    "            lexica[l] = pd.read_csv(filepath)\n",
    "\n",
    "        # create centroids\n",
    "        all_centroids = {}        \n",
    "        for l in languages:\n",
    "            categories = lexica[l][\"CATEGORY\"].unique()\n",
    "            centroids = {}\n",
    "            for c in categories:\n",
    "                words = lexica[l][lexica[l][\"CATEGORY\"] == c][\"word\"].tolist()\n",
    "                embeddings = self.model.encode(words)\n",
    "                centroid = np.mean(embeddings, axis=0)\n",
    "                centroids[c] = centroid\n",
    "            assert len(categories) == len(centroids.keys())\n",
    "            all_centroids[l] = centroids\n",
    "            print(f\"Centroids for {l} created.\")\n",
    "        return all_centroids\n",
    "\n",
    "    # input: list of words\n",
    "    def calculate_single_group_alignment(self, group:list, language:str=\"english\"):\n",
    "        #find max avg cos sim between word embeddings and centroids\n",
    "        category_similarities = {}\n",
    "        centroids = self.centroids[language]\n",
    "        for category, centroid_emb in centroids.items():\n",
    "            #calculate cosine similarity\n",
    "            cos_sim = []\n",
    "            for word in group:\n",
    "                word_emb = self.model.encode(word)\n",
    "                cos_sim.append(np.dot(word_emb, centroid_emb) / (np.linalg.norm(word_emb) * np.linalg.norm(centroid_emb)))\n",
    "            avg_cos_sim = np.mean(cos_sim)\n",
    "            category_similarities[category] = avg_cos_sim\n",
    "        #return highest similarity score\n",
    "        return max(category_similarities.values())\n",
    "\n",
    "    def calculate_group_alignment(self, groups:list, language:str=\"english\"):\n",
    "        group_alignments = []\n",
    "        for group in groups:\n",
    "            group_alignments.append(self.calculate_single_group_alignment(group, language))\n",
    "        return group_alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Expert Alignment Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids for english created.\n",
      "Centroids for spanish created.\n",
      "Centroids for chinese created.\n",
      "Centroids for japanese created.\n",
      "Group: ['hello', 'goodbye', 'please'], Alignment: 0.7011184692382812\n",
      "Group: ['computer', 'laptop', 'phone'], Alignment: 0.4826013147830963\n",
      "Group: ['idiot', 'stupid', 'dumb'], Alignment: 0.7102837562561035\n",
      "Group: ['thank you', 'grateful', 'thanks'], Alignment: 0.9256609082221985\n"
     ]
    }
   ],
   "source": [
    "metric = Metric()\n",
    "sample_groups = [ [\"hello\", \"goodbye\", \"please\"], \n",
    "                [\"computer\", \"laptop\", \"phone\"], \n",
    "                [\"idiot\", \"stupid\", \"dumb\"], \n",
    "                [\"thank you\", \"grateful\", \"thanks\"]]\n",
    "alignments = metric.calculate_group_alignment(sample_groups)\n",
    "for group, alignment in zip(sample_groups, alignments):\n",
    "    print(f\"Group: {group}, Alignment: {alignment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Word Level Groups ----\n",
      "[['That'], ['is'], ['why'], ['he'], ['is'], ['a'], ['‘pretender’.'], ['He'], ['has'], ['never'], ['claimed'], ['to'], ['be'], ['a'], ['King'], ['-'], ['or'], ['a'], ['Kaiser,'], ['for'], ['that'], ['matter.'], ['He'], ['is'], ['in'], ['the'], ['same'], ['class'], ['as'], ['the'], ['Comte'], ['de'], ['Paris,'], ['who'], ['is'], ['not'], ['the'], ['King'], ['of'], ['France,'], ['but'], ['would'], ['be'], ['if'], ['the'], ['Bourbons'], ['were'], ['placed'], ['on'], ['a'], ['restored'], ['French'], ['throne.']]\n",
      "0.71663177\n",
      "[[\"Let's\"], ['knock'], ['any'], [\"'EngVar'\"], ['shenanigans'], ['on'], ['the'], ['head'], ['right'], ['away,'], ['shall'], ['we?'], ['The'], ['Manual'], ['of'], ['Style,'], ['as'], ['I'], ['understand'], ['it,'], ['makes'], ['it'], ['clear'], ['that'], ['the'], [\"subject's\"], ['national'], ['ties'], ['and'], ['own'], ['language'], ['set'], ['the'], ['course.']]\n",
      "0.7128271\n",
      "[['Thank'], ['you'], ['for'], ['your'], ['contributions.'], ['There'], ['are'], ['some'], ['conventions'], ['that'], ['apply'], ['to'], ['articles,'], ['and'], ['medical'], ['articles'], ['in'], ['particular.'], ['Secondary'], ['sources'], ['were'], ['available'], ['for'], ['the'], ['material,'], ['and'], ['should'], ['be'], ['cited'], ['to'], ['validate'], ['the'], ['medical'], ['information'], ['from'], ['the'], ['studies.']]\n",
      "0.68416727\n",
      "[['The'], ['conversion'], ['of'], ['tacit'], ['to'], ['explicit'], ['knowledge'], ['is'], ['seen'], ['in'], ['for'], ['example'], ['the'], ['bread'], ['making'], [\"machine's\"], ['case.'], ['In'], ['response'], ['to'], ['your'], ['question,'], ['culture'], ['is'], ['a'], ['broad'], ['term.'], ['I'], ['would'], ['like'], ['to'], ['narrow'], ['it'], ['down'], ['to'], ['organization'], ['culture.']]\n",
      "0.69440025\n",
      "\n",
      "---- Phrase Level Groups ----\n",
      "[['That', 'is', 'why'], ['he', 'is', 'a'], ['‘pretender’.', 'He', 'has'], ['never', 'claimed', 'to'], ['be', 'a', 'King'], ['-', 'or', 'a'], ['Kaiser,', 'for', 'that'], ['matter.', 'He', 'is'], ['in', 'the', 'same'], ['class', 'as', 'the'], ['Comte', 'de', 'Paris,'], ['who', 'is', 'not'], ['the', 'King', 'of'], ['France,', 'but', 'would'], ['be', 'if', 'the'], ['Bourbons', 'were', 'placed'], ['on', 'a', 'restored'], ['French', 'throne.']]\n",
      "0.6799007\n",
      "[[\"Let's\", 'knock', 'any'], [\"'EngVar'\", 'shenanigans', 'on'], ['the', 'head', 'right'], ['away,', 'shall', 'we?'], ['The', 'Manual', 'of'], ['Style,', 'as', 'I'], ['understand', 'it,', 'makes'], ['it', 'clear', 'that'], ['the', \"subject's\", 'national'], ['ties', 'and', 'own'], ['language', 'set', 'the'], ['course.']]\n",
      "0.6641653\n",
      "[['Thank', 'you', 'for'], ['your', 'contributions.', 'There'], ['are', 'some', 'conventions'], ['that', 'apply', 'to'], ['articles,', 'and', 'medical'], ['articles', 'in', 'particular.'], ['Secondary', 'sources', 'were'], ['available', 'for', 'the'], ['material,', 'and', 'should'], ['be', 'cited', 'to'], ['validate', 'the', 'medical'], ['information', 'from', 'the'], ['studies.']]\n",
      "0.6234655\n",
      "[['The', 'conversion', 'of'], ['tacit', 'to', 'explicit'], ['knowledge', 'is', 'seen'], ['in', 'for', 'example'], ['the', 'bread', 'making'], [\"machine's\", 'case.', 'In'], ['response', 'to', 'your'], ['question,', 'culture', 'is'], ['a', 'broad', 'term.'], ['I', 'would', 'like'], ['to', 'narrow', 'it'], ['down', 'to', 'organization'], ['culture.']]\n",
      "0.6278292\n",
      "\n",
      "---- Sentence Level Groups ----\n",
      "[['That', 'is', 'why', 'he', 'is', 'a', '‘pretender’.'], ['He', 'has', 'never', 'claimed', 'to', 'be', 'a', 'King', '-', 'or', 'a', 'Kaiser,', 'for', 'that', 'matter.'], ['He', 'is', 'in', 'the', 'same', 'class', 'as', 'the', 'Comte', 'de', 'Paris,', 'who', 'is', 'not', 'the', 'King', 'of', 'France,', 'but', 'would', 'be', 'if', 'the', 'Bourbons', 'were', 'placed', 'on', 'a', 'restored', 'French', 'throne.']]\n",
      "0.6752072\n",
      "[[\"Let's\", 'knock', 'any', \"'EngVar'\", 'shenanigans', 'on', 'the', 'head', 'right', 'away,', 'shall', 'we?'], ['The', 'Manual', 'of', 'Style,', 'as', 'I', 'understand', 'it,', 'makes', 'it', 'clear', 'that', 'the', \"subject's\", 'national', 'ties', 'and', 'own', 'language', 'set', 'the', 'course.']]\n",
      "0.6275226\n",
      "[['Thank', 'you', 'for', 'your', 'contributions.'], ['There', 'are', 'some', 'conventions', 'that', 'apply', 'to', 'articles,', 'and', 'medical', 'articles', 'in', 'particular.'], ['Secondary', 'sources', 'were', 'available', 'for', 'the', 'material,', 'and', 'should', 'be', 'cited', 'to', 'validate', 'the', 'medical', 'information', 'from', 'the', 'studies.']]\n",
      "0.62612367\n",
      "[['The', 'conversion', 'of', 'tacit', 'to', 'explicit', 'knowledge', 'is', 'seen', 'in', 'for', 'example', 'the', 'bread', 'making', \"machine's\", 'case.'], ['In', 'response', 'to', 'your', 'question,', 'culture', 'is', 'a', 'broad', 'term.'], ['I', 'would', 'like', 'to', 'narrow', 'it', 'down', 'to', 'organization', 'culture.']]\n",
      "0.62644404\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader: \n",
    "    word_lists = batch['word_list']\n",
    "    word_lists = list(map(list, zip(*word_lists)))\n",
    "    processed_word_lists = []\n",
    "    for word_list in word_lists:\n",
    "        processed_word_lists.append([word for word in word_list if word != ''])\n",
    "    print(\"---- Word Level Groups ----\")\n",
    "    word_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        word_groups = []\n",
    "        for word in word_list:\n",
    "            word_groups.append([word])\n",
    "        print(word_groups)\n",
    "        alignments = metric.calculate_group_alignment(word_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    print(\"\\n---- Phrase Level Groups ----\")\n",
    "    phrase_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        phrase_groups = []\n",
    "        #each group is 3 consecutive words\n",
    "        for i in range(0, len(word_list), 3):\n",
    "            phrase_groups.append(word_list[i:i+3])\n",
    "        print(phrase_groups)\n",
    "        alignments = metric.calculate_group_alignment(phrase_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    print(\"\\n---- Sentence Level Groups ----\")\n",
    "    sentence_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        sentence_groups = []\n",
    "\n",
    "        #reconstruct sentences from word list\n",
    "        sentence = \"\"\n",
    "        for word in word_list:\n",
    "            sentence += word + \" \"\n",
    "            if word[-1] == \".\" or word[-1] == \"!\" or word[-1] == \"?\":\n",
    "                sentence_groups.append(sentence.split())\n",
    "                sentence = \"\"\n",
    "        if(len(sentence) > 0):\n",
    "            sentence_groups.append(sentence.split())\n",
    "\n",
    "        print(sentence_groups)\n",
    "        alignments = metric.calculate_group_alignment(sentence_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtop",
   "language": "python",
   "name": "vtop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
