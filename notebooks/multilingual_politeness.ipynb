{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'projection_helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprojection_helper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m project_points_onto_axes\n\u001b[1;32m     18\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'projection_helper'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import TextClassificationPipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "from transformers import Pipeline\n",
    "from torch import Tensor \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "import sentence_transformers\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_REPO = \"shreyahavaldar/multilingual_politeness\"\n",
    "MODEL_REPO = \"shreyahavaldar/xlm-roberta-politeness\"\n",
    "\n",
    "def load_data():\n",
    "    hf_dataset = load_dataset(DATASET_REPO)\n",
    "    #convert to torch dataset\n",
    "    hf_dataset.set_format(type='torch')\n",
    "    return hf_dataset\n",
    "\n",
    "def load_model():\n",
    "    model = AutoModel.from_pretrained(MODEL_REPO)\n",
    "    #convert to pytorch model\n",
    "    torch_model = nn.Sequential(model, nn.Linear(model.config.hidden_size, 1))\n",
    "    model.to(device)\n",
    "    return torch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Alignment Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "\n",
    "def project_points_onto_axes(points, x_point1, x_point2, y_point1, y_point2):\n",
    "    # Compute the unit vectors along the axes\n",
    "    x_axis_vector = (x_point2 - x_point1) / 2\n",
    "    y_axis_vector = (y_point2 - y_point1) / 2\n",
    "    x_axis_vector = x_axis_vector / np.linalg.norm(x_axis_vector, ord=2)\n",
    "    y_axis_vector = y_axis_vector / np.linalg.norm(y_axis_vector, ord=2)\n",
    "    # Now length of the vector is 1\n",
    "    cos = np.dot(x_axis_vector,y_axis_vector)\n",
    "    # Project each point onto the x-axis and y-axis\n",
    "    x_projection = []\n",
    "    y_projection = []\n",
    "    x_dist = []\n",
    "    y_dist = []\n",
    "    x_middle = (x_point1 + x_point2) / 2\n",
    "    y_middle = (y_point1 + y_point2) / 2\n",
    "    x1x = np.dot(x_point1 - x_middle , x_axis_vector) \n",
    "    x2x = np.dot(x_point2 - x_middle, x_axis_vector) \n",
    "    y1y = np.dot(y_point1 - y_middle, y_axis_vector) \n",
    "    y2y = np.dot(y_point2 - y_middle, y_axis_vector) \n",
    "    x1y = np.dot(x_point1 - y_middle, y_axis_vector) \n",
    "    x2y = np.dot(x_point2 - y_middle, y_axis_vector) \n",
    "    y1x = np.dot(y_point1 - x_middle, x_axis_vector) \n",
    "    y2x = np.dot(y_point2 - x_middle, x_axis_vector) \n",
    "    x1xtrue = x1x - x1y*cos\n",
    "    x1ytrue = x1y - x1x*cos\n",
    "    x2xtrue = x2x - x2y*cos\n",
    "    x2ytrue = x2y - x2x*cos\n",
    "    y1xtrue = y1x - y1y*cos\n",
    "    y1ytrue = y1y - y1x*cos\n",
    "    y2xtrue = y2x - y2y*cos\n",
    "    y2ytrue = y2y - y2x*cos\n",
    "    xorigin, yorigin = line_intersection((x1xtrue,x2xtrue,y1xtrue,y2xtrue), (x1ytrue,x2ytrue,y1ytrue,y2ytrue))\n",
    "    x_negative_scale = np.abs(x1xtrue - xorigin)\n",
    "    x_positive_scale = np.abs(x2xtrue - xorigin)\n",
    "    y_negative_scale = np.abs(y1ytrue - yorigin)\n",
    "    y_positive_scale = np.abs(y2ytrue - yorigin)\n",
    "    for point in points:\n",
    "        x_proj = np.dot(point - x_middle , x_axis_vector)  \n",
    "        y_proj = np.dot(point - y_middle , y_axis_vector) \n",
    "        true_y = (y_proj - x_proj*cos) - yorigin\n",
    "        true_x = (x_proj - y_proj*cos) - xorigin\n",
    "        if true_x < 0:\n",
    "            true_x = true_x / x_negative_scale\n",
    "        else:\n",
    "            true_x = true_x / x_positive_scale\n",
    "        if true_y < 0:\n",
    "            true_y = true_y / y_negative_scale\n",
    "        else:\n",
    "            true_y = true_y / y_positive_scale\n",
    "        x_projection.append(true_x)\n",
    "        y_projection.append(true_y)\n",
    "        x_dist.append(np.linalg.norm(point - x_proj*x_axis_vector, ord=2))\n",
    "        y_dist.append(np.linalg.norm(point - y_proj*y_axis_vector, ord=2))\n",
    "\n",
    "    # Return the magnitudes of the projections as numpy arrays\n",
    "    return np.array(x_projection), np.array(y_projection), np.array(x_dist), np.array(y_dist)\n",
    "\n",
    "def line_intersection(x_pts, y_pts):  \n",
    "    line1 = ((x_pts[0], y_pts[0]), (x_pts[1], y_pts[1]))\n",
    "    line2 = ((x_pts[2], y_pts[2]), (x_pts[3], y_pts[3]))\n",
    "    xdiff = (line1[0][0] - line1[1][0], line2[0][0] - line2[1][0])\n",
    "    ydiff = (line1[0][1] - line1[1][1], line2[0][1] - line2[1][1])\n",
    "\n",
    "    def det(a, b):\n",
    "        return a[0] * b[1] - a[1] * b[0]\n",
    "\n",
    "    div = det(xdiff, ydiff)\n",
    "    if div == 0:\n",
    "       raise Exception('lines do not intersect')\n",
    "\n",
    "    d = (det(*line1), det(*line2))\n",
    "    x = det(d, xdiff) / div\n",
    "    y = det(d, ydiff) / div\n",
    "    return x, y\n",
    "\n",
    "\n",
    "class Metric(nn.Module): \n",
    "    def __init__(self, model_name:str=\"distiluse-base-multilingual-cased\"): \n",
    "        super(Metric, self).__init__()\n",
    "        self.model = sentence_transformers.SentenceTransformer(model_name)\n",
    "        self.centroids = self.get_centroids()\n",
    "    \n",
    "    def get_centroids(self):\n",
    "        # read lexica files\n",
    "        languages = [\"english\", \"spanish\", \"chinese\", \"japanese\"]\n",
    "        lexica = {}\n",
    "        for l in languages:\n",
    "            filepath = f\"../src/exlib/utils/politeness_lexica/{l}_politelex.csv\"\n",
    "            lexica[l] = pd.read_csv(filepath)\n",
    "\n",
    "        # create centroids\n",
    "        all_centroids = {}        \n",
    "        for l in languages:\n",
    "            categories = lexica[l][\"CATEGORY\"].unique()\n",
    "            centroids = {}\n",
    "            for c in categories:\n",
    "                words = lexica[l][lexica[l][\"CATEGORY\"] == c][\"word\"].tolist()\n",
    "                embeddings = self.model.encode(words)\n",
    "                centroid = np.mean(embeddings, axis=0)\n",
    "                centroids[c] = centroid\n",
    "            assert len(categories) == len(centroids.keys())\n",
    "            all_centroids[l] = centroids\n",
    "            print(f\"Centroids for {l} created.\")\n",
    "        return all_centroids\n",
    "\n",
    "    # input: list of words\n",
    "    def calculate_single_group_alignment(self, group:list, language:str=\"english\"):\n",
    "        #find max avg cos sim between word embeddings and centroids\n",
    "        category_similarities = {}\n",
    "        centroids = self.centroids[language]\n",
    "        for category, centroid_emb in centroids.items():\n",
    "            #calculate cosine similarity\n",
    "            cos_sim = []\n",
    "            for word in group:\n",
    "                word_emb = self.model.encode(word)\n",
    "                cos_sim.append(np.dot(word_emb, centroid_emb) / (np.linalg.norm(word_emb) * np.linalg.norm(centroid_emb)))\n",
    "            avg_cos_sim = np.mean(cos_sim)\n",
    "            category_similarities[category] = avg_cos_sim\n",
    "        #return highest similarity score\n",
    "        return max(category_similarities.values())\n",
    "\n",
    "    def calculate_group_alignment(self, groups:list, language:str=\"english\"):\n",
    "        group_alignments = []\n",
    "        for group in groups:\n",
    "            group_alignments.append(self.calculate_single_group_alignment(group, language))\n",
    "        return group_alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Group Alignment Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids for english created.\n",
      "Centroids for spanish created.\n",
      "Centroids for chinese created.\n",
      "Centroids for japanese created.\n",
      "Group: ['dog', 'cat', 'fish'], Alignment: 0.5292773842811584\n",
      "Group: ['hello', 'goodbye', 'please'], Alignment: 0.7011184692382812\n",
      "Group: ['computer', 'laptop', 'phone'], Alignment: 0.4826013147830963\n",
      "Group: ['idiot', 'stupid', 'dumb'], Alignment: 0.7102837562561035\n",
      "Group: ['thank you', 'grateful', 'thanks'], Alignment: 0.9256609082221985\n"
     ]
    }
   ],
   "source": [
    "metric = Metric()\n",
    "sample_groups = [[\"dog\", \"cat\", \"fish\"], \n",
    "                [\"hello\", \"goodbye\", \"please\"], \n",
    "                [\"computer\", \"laptop\", \"phone\"], \n",
    "                [\"idiot\", \"stupid\", \"dumb\"], \n",
    "                [\"thank you\", \"grateful\", \"thanks\"]]\n",
    "alignments = metric.calculate_group_alignment(sample_groups)\n",
    "for group, alignment in zip(sample_groups, alignments):\n",
    "    print(f\"Group: {group}, Alignment: {alignment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtop",
   "language": "python",
   "name": "vtop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
