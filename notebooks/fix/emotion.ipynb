{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment line below to install exlib\n",
    "# !pip install exlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "import sentence_transformers\n",
    "\n",
    "import exlib\n",
    "from exlib.utils.emotion_helper import project_points_onto_axes, load_emotions\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_REPO = \"BrachioLab/roberta-base-go_emotions\"\n",
    "DATASET_REPO = \"BrachioLab/emotion\"\n",
    "TOKENIZER_REPO = \"roberta-base\"\n",
    "\n",
    "def load_data():\n",
    "    hf_dataset = load_dataset(DATASET_REPO)\n",
    "    return hf_dataset\n",
    "\n",
    "def load_model():\n",
    "    model = AutoModel.from_pretrained(MODEL_REPO)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "#go emotions dataset\n",
    "class EmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        dataset = load_dataset(DATASET_REPO)[split]        \n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_REPO)\n",
    "        self.max_len = max([len(text.split()) for text in dataset['text']])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        label = self.dataset[idx]['labels'][0]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        word_list = text.split()\n",
    "        for i in range(len(word_list), self.max_len):\n",
    "            word_list.append('')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label),\n",
    "            'word_list': word_list\n",
    "        }\n",
    "\n",
    "#classifier for go emotions dataset\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(MODEL_REPO)\n",
    "        self.classifier = nn.Linear(768, 28)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        cls_token = last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_token)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample inference on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at shreyahavaldar/roberta-base-go_emotions and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/10853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My favourite food is anything I didn't have to cook myself.\n",
      "Emotion: 5\n",
      "\n",
      "Text: Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead\n",
      "Emotion: 21\n",
      "\n",
      "Text: WHY THE FUCK IS BAYLESS ISOING\n",
      "Emotion: 19\n",
      "\n",
      "Text: To make her feel threatened\n",
      "Emotion: 11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = EmotionDataset(\"train\")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "model = EmotionClassifier()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(dataloader): \n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    utterances = [dataset.tokenizer.decode(input_id, skip_special_tokens=True) for input_id in input_ids]\n",
    "    for utterance, label in zip(utterances, output):\n",
    "        print(\"Text: {}\\nEmotion: {}\\n\".format(utterance, label.argmax()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Alignment Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(nn.Module): \n",
    "    def __init__(self, model_name:str=\"all-mpnet-base-v2\"): \n",
    "        super(Metric, self).__init__()\n",
    "        self.model = sentence_transformers.SentenceTransformer(model_name)\n",
    "        points = self.define_circumplex()\n",
    "        self.x1 = points[0]\n",
    "        self.x2 = points[1]\n",
    "        self.y1 = points[3]\n",
    "        self.y2 = points[2]\n",
    "\n",
    "    def define_circumplex(self):\n",
    "        emotions = load_emotions()\n",
    "        axis_labels = [\"NV\", \"PV\", \"HA\", \"LA\"]\n",
    "        axis_points = []\n",
    "        for label in axis_labels:\n",
    "            emotion_words = emotions[label]\n",
    "            emotion_embeddings = self.model.encode(emotion_words)\n",
    "            axis_points.append(np.mean(emotion_embeddings, axis=0))\n",
    "        return axis_points\n",
    "    \n",
    "    def distance_from_circumplex(self, embeddings):\n",
    "        projection = project_points_onto_axes(embeddings, self.x1, self.x2, self.y1, self.y2)\n",
    "        x_projections = projection[0]\n",
    "        y_projections = projection[1]\n",
    "        distances = []\n",
    "        for x, y in zip(x_projections, y_projections):                \n",
    "            distances.append(np.abs(np.sqrt(x**2 + y**2)-1))\n",
    "        return 1/np.mean(distances)\n",
    "\n",
    "    def mean_pairwise_dist(self, embeddings):\n",
    "        projection = project_points_onto_axes(embeddings, self.x1, self.x2, self.y1, self.y2)\n",
    "        distances = []\n",
    "        x_coords = projection[0]\n",
    "        y_coords = projection[1]\n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(i+1, len(embeddings)):\n",
    "                x_dist = x_coords[i] - x_coords[j]\n",
    "                y_dist = y_coords[i] - y_coords[j]\n",
    "                distances.append(np.sqrt(x_dist**2 + y_dist**2))\n",
    "        return 1/np.mean(distances)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    # input: list of words\n",
    "    def calculate_group_alignment(self, groups:list, language:str=\"english\"):\n",
    "        alignments = []\n",
    "        for group in groups:\n",
    "            embeddings = self.model.encode(group)\n",
    "            circumplex_dist = self.distance_from_circumplex(embeddings)\n",
    "            if(len(embeddings) == 1): \n",
    "                alignments.append(circumplex_dist)\n",
    "            else:\n",
    "                mean_dist = self.mean_pairwise_dist(embeddings)\n",
    "                combined_dist = circumplex_dist*mean_dist\n",
    "                alignments.append(combined_dist)\n",
    "        return alignments\n",
    "    \n",
    "    def forward(self, group_masks:list, original_data:EmotionDataset):\n",
    "        #create groups\n",
    "        groups = []\n",
    "        for i in range(len(group_masks)):\n",
    "            word_list_ex = original_data[i]['word_list']\n",
    "            mask = group_masks[i]\n",
    "            print(word_list_ex, mask)\n",
    "            group = [word_list_ex[j] for j in range(len(mask)) if mask[j] == 1]\n",
    "            groups.append(group)\n",
    "        return np.mean(self.calculate_group_alignment(groups, language))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Expert Alignment Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: ['hooray!', 'yay!', 'surprise!'], Alignment: 4.486954104489469\n",
      "Group: ['happy', 'excited'], Alignment: 8.559297785723178\n",
      "Group: ['beautiful', 'ugly'], Alignment: 2.1811178776545157\n"
     ]
    }
   ],
   "source": [
    "metric = Metric()\n",
    "sample_groups = [[\"hooray!\", \"yay!\", \"surprise!\"], \n",
    "                [\"happy\", \"excited\"],\n",
    "                [\"beautiful\", \"ugly\"]]\n",
    "                \n",
    "alignments = metric.calculate_group_alignment(sample_groups)\n",
    "\n",
    "for group, alignment in zip(sample_groups, alignments):\n",
    "    print(f\"Group: {group}, Alignment: {alignment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Word Level Groups ----\n",
      "[['My'], ['favourite'], ['food'], ['is'], ['anything'], ['I'], [\"didn't\"], ['have'], ['to'], ['cook'], ['myself.']]\n",
      "1.2176552995190102\n",
      "[['Now'], ['if'], ['he'], ['does'], ['off'], ['himself,'], ['everyone'], ['will'], ['think'], ['hes'], ['having'], ['a'], ['laugh'], ['screwing'], ['with'], ['people'], ['instead'], ['of'], ['actually'], ['dead']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2696059762679086\n",
      "[['WHY'], ['THE'], ['FUCK'], ['IS'], ['BAYLESS'], ['ISOING']]\n",
      "1.2578319227761996\n",
      "[['To'], ['make'], ['her'], ['feel'], ['threatened']]\n",
      "1.877290266531395\n",
      "\n",
      "---- Phrase Level Groups ----\n",
      "[['My', 'favourite', 'food'], ['is', 'anything', 'I'], [\"didn't\", 'have', 'to'], ['cook', 'myself.']]\n",
      "5.190699789259442\n",
      "[['Now', 'if', 'he'], ['does', 'off', 'himself,'], ['everyone', 'will', 'think'], ['hes', 'having', 'a'], ['laugh', 'screwing', 'with'], ['people', 'instead', 'of'], ['actually', 'dead']]\n",
      "5.836899288742894\n",
      "[['WHY', 'THE', 'FUCK'], ['IS', 'BAYLESS', 'ISOING']]\n",
      "4.158418026776409\n",
      "[['To', 'make', 'her'], ['feel', 'threatened']]\n",
      "5.475612871815331\n",
      "\n",
      "---- Sentence Level Groups ----\n",
      "[['My', 'favourite', 'food', 'is', 'anything', 'I', \"didn't\", 'have', 'to', 'cook', 'myself.']]\n",
      "4.9537909932648825\n",
      "[['Now', 'if', 'he', 'does', 'off', 'himself,', 'everyone', 'will', 'think', 'hes', 'having', 'a', 'laugh', 'screwing', 'with', 'people', 'instead', 'of', 'actually', 'dead']]\n",
      "4.483586837664701\n",
      "[['WHY', 'THE', 'FUCK', 'IS', 'BAYLESS', 'ISOING']]\n",
      "4.559812480172151\n",
      "[['To', 'make', 'her', 'feel', 'threatened']]\n",
      "3.041145133084334\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader: \n",
    "    word_lists = batch['word_list']\n",
    "    word_lists = list(map(list, zip(*word_lists)))\n",
    "    processed_word_lists = []\n",
    "    for word_list in word_lists:\n",
    "        processed_word_lists.append([word for word in word_list if word != ''])\n",
    "    print(\"---- Word Level Groups ----\")\n",
    "    word_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        word_groups = []\n",
    "        for word in word_list:\n",
    "            word_groups.append([word])\n",
    "        print(word_groups)\n",
    "        alignments = metric.calculate_group_alignment(word_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    print(\"\\n---- Phrase Level Groups ----\")\n",
    "    phrase_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        phrase_groups = []\n",
    "        #each group is 3 consecutive words\n",
    "        for i in range(0, len(word_list), 3):\n",
    "            phrase_groups.append(word_list[i:i+3])\n",
    "        print(phrase_groups)\n",
    "        alignments = metric.calculate_group_alignment(phrase_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    print(\"\\n---- Sentence Level Groups ----\")\n",
    "    sentence_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        sentence_groups = []\n",
    "\n",
    "        #reconstruct sentences from word list\n",
    "        sentence = \"\"\n",
    "        for word in word_list:\n",
    "            sentence += word + \" \"\n",
    "            if word[-1] == \".\" or word[-1] == \"!\" or word[-1] == \"?\":\n",
    "                sentence_groups.append(sentence.split())\n",
    "                sentence = \"\"\n",
    "        if(len(sentence) > 0):\n",
    "            sentence_groups.append(sentence.split())\n",
    "\n",
    "        print(sentence_groups)\n",
    "        alignments = metric.calculate_group_alignment(sentence_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
