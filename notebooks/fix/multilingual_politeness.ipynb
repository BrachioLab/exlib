{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import sentence_transformers\n",
    "\n",
    "import exlib\n",
    "from exlib.utils.politeness_helper import load_lexica\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_REPO = \"shreyahavaldar/multilingual_politeness\"\n",
    "MODEL_REPO = \"shreyahavaldar/xlm-roberta-politeness\"\n",
    "TOKENIZER_REPO = \"xlm-roberta-base\"\n",
    "\n",
    "def load_data():\n",
    "    hf_dataset = load_dataset(DATASET_REPO)\n",
    "    return hf_dataset\n",
    "\n",
    "def load_model():\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(MODEL_REPO)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "class PolitenessDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split, language=\"english\"):\n",
    "        dataset = load_dataset(DATASET_REPO)[split]\n",
    "        dataset = dataset.filter(lambda x: x[\"language\"] == language)\n",
    "        dataset = dataset.rename_column(\"politeness\", \"label\")\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(TOKENIZER_REPO)\n",
    "        self.max_len = max([len(text.split()) for text in dataset['Utterance']])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[\"Utterance\"][idx]\n",
    "        label = self.dataset[\"label\"][idx]\n",
    "        encoding = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "        word_list = text.split()\n",
    "        for i in range(len(word_list), self.max_len):\n",
    "            word_list.append('')\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"label\": torch.tensor(label),\n",
    "            'word_list': word_list\n",
    "        }\n",
    "\n",
    "class PolitenessClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolitenessClassifier, self).__init__()\n",
    "        self.model = load_model()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        logits = outputs.logits\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample inference on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: That is why he is a ‘pretender’. He has never claimed to be a King - or a Kaiser, for that matter. He is in the same class as the Comte de Paris, who is not the King of France, but would be if the Bourbons were placed on a restored French throne.\n",
      "Politeness: -0.11096465587615967\n",
      "\n",
      "Text: Let's knock any 'EngVar' shenanigans on the head right away, shall we? The Manual of Style, as I understand it, makes it clear that the subject's national ties and own language set the course.\n",
      "Politeness: -0.4824700355529785\n",
      "\n",
      "Text: Thank you for your contributions. There are some conventions that apply to articles, and medical articles in particular. Secondary sources were available for the material, and should be cited to validate the medical information from the studies.\n",
      "Politeness: 1.5924450159072876\n",
      "\n",
      "Text: The conversion of tacit to explicit knowledge is seen in for example the bread making machine's case. In response to your question, culture is a broad term. I would like to narrow it down to organization culture.\n",
      "Politeness: 0.7522311210632324\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = PolitenessDataset(\"train\")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "model = PolitenessClassifier()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(dataloader): \n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    utterances = [dataset.tokenizer.decode(input_id, skip_special_tokens=True) for input_id in input_ids]\n",
    "    for utterance, label in zip(utterances, output):\n",
    "        print(\"Text: {}\\nPoliteness: {}\\n\".format(utterance, label.item()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Alignment Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(nn.Module): \n",
    "    def __init__(self, model_name:str=\"distiluse-base-multilingual-cased\"): \n",
    "        super(Metric, self).__init__()\n",
    "        self.model = sentence_transformers.SentenceTransformer(model_name)\n",
    "        self.centroids = self.get_centroids()\n",
    "    \n",
    "    def get_centroids(self):\n",
    "        # read lexica files\n",
    "        languages = [\"english\", \"spanish\", \"chinese\", \"japanese\"]\n",
    "        lexica = {}\n",
    "        for l in languages:\n",
    "            lexica[l] = load_lexica(l)\n",
    "\n",
    "        # create centroids\n",
    "        all_centroids = {}        \n",
    "        for l in languages:\n",
    "            categories = lexica[l][\"CATEGORY\"].unique()\n",
    "            centroids = {}\n",
    "            for c in categories:\n",
    "                words = lexica[l][lexica[l][\"CATEGORY\"] == c][\"word\"].tolist()\n",
    "                embeddings = self.model.encode(words)\n",
    "                centroid = np.mean(embeddings, axis=0)\n",
    "                centroids[c] = centroid\n",
    "            assert len(categories) == len(centroids.keys())\n",
    "            all_centroids[l] = centroids\n",
    "            print(f\"Centroids for {l} created.\")\n",
    "        return all_centroids\n",
    "\n",
    "    # input: list of words\n",
    "    def calculate_single_group_alignment(self, group:list, language:str=\"english\"):\n",
    "        #find max avg cos sim between word embeddings and centroids\n",
    "        category_similarities = {}\n",
    "        centroids = self.centroids[language]\n",
    "        for category, centroid_emb in centroids.items():\n",
    "            #calculate cosine similarity\n",
    "            cos_sim = []\n",
    "            for word in group:\n",
    "                word_emb = self.model.encode(word)\n",
    "                cos_sim.append(np.dot(word_emb, centroid_emb) / (np.linalg.norm(word_emb) * np.linalg.norm(centroid_emb)))\n",
    "            avg_cos_sim = np.mean(cos_sim)\n",
    "            category_similarities[category] = avg_cos_sim\n",
    "        #return highest similarity score\n",
    "        return max(category_similarities.values())\n",
    "\n",
    "    # input: list of words\n",
    "    def calculate_single_group_alignment(self, group:list, language:str=\"english\"):\n",
    "        #find max avg cos sim between word embeddings and centroids\n",
    "        category_similarities = {}\n",
    "        centroids = self.centroids[language]\n",
    "        word_embs = []\n",
    "        for word in group:\n",
    "            word_emb = self.model.encode(word)\n",
    "            word_embs.append(torch.tensor(word_emb))\n",
    "\n",
    "        # word_embs = self.model.encode(group)\n",
    "        word_embs = torch.stack(word_embs).to(device)\n",
    "        word_emb_pt = torch.tensor(word_embs).to(device)\n",
    "        centroid_embs = list(centroids.values())\n",
    "        centroid_emb_pt = torch.tensor(centroid_embs).to(device)\n",
    "\n",
    "        # Compute the norms for each batch\n",
    "        norm_word = torch.norm(word_emb_pt, dim=1, keepdim=True)  # Shape becomes (n, 1)\n",
    "        norm_centroid = torch.norm(centroid_emb_pt, dim=1, keepdim=True)  # Shape becomes (m, 1)\n",
    "\n",
    "        # Compute the dot products\n",
    "        # Transpose centroid_emb_pt to make it (d, m) for matrix multiplication\n",
    "        dot_product = torch.mm(word_emb_pt, centroid_emb_pt.T)  # Resulting shape is (n, m)\n",
    "\n",
    "        # Compute the outer product of the norms\n",
    "        norms_product = torch.mm(norm_word, norm_centroid.T)  # Resulting shape is (n, m)\n",
    "\n",
    "        # Calculate the cosine similarity matrix\n",
    "        cosine_similarity = dot_product / norms_product\n",
    "\n",
    "        group_alignment = cosine_similarity.mean(0).max().item()\n",
    "        return group_alignment\n",
    "\n",
    "    def calculate_group_alignment(self, groups:list, language:str=\"english\"):\n",
    "        group_alignments = []\n",
    "        for group in groups:\n",
    "            group_alignments.append(self.calculate_single_group_alignment(group, language))\n",
    "        return group_alignments\n",
    "    \n",
    "    def forward(self, group_masks:list, original_data:PolitenessDataset, language=\"english\"):\n",
    "        #create groups\n",
    "        groups = []\n",
    "        for i in range(len(group_masks)):\n",
    "            word_list_ex = original_data[i]['word_list']\n",
    "            mask = group_masks[i]\n",
    "            print(word_list_ex, mask)\n",
    "            group = [word_list_ex[j] for j in range(len(mask)) if mask[j] == 1]\n",
    "            groups.append(group)\n",
    "        return np.mean(self.calculate_group_alignment(groups, language))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Expert Alignment Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids for english created.\n",
      "Centroids for spanish created.\n",
      "Centroids for chinese created.\n",
      "Centroids for japanese created.\n",
      "Group: ['hello', 'goodbye', 'please'], Alignment: 0.7011186480522156\n",
      "Group: ['computer', 'laptop', 'phone'], Alignment: 0.4826011657714844\n",
      "Group: ['idiot', 'stupid', 'dumb'], Alignment: 0.7102838158607483\n",
      "Group: ['thank you', 'grateful', 'thanks'], Alignment: 0.9256609082221985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_326434/650531704.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_emb_pt = torch.tensor(word_embs).to(device)\n"
     ]
    }
   ],
   "source": [
    "metric = Metric()\n",
    "sample_groups = [ [\"hello\", \"goodbye\", \"please\"], \n",
    "                [\"computer\", \"laptop\", \"phone\"], \n",
    "                [\"idiot\", \"stupid\", \"dumb\"], \n",
    "                [\"thank you\", \"grateful\", \"thanks\"]]\n",
    "alignments = metric.calculate_group_alignment(sample_groups)\n",
    "for group, alignment in zip(sample_groups, alignments):\n",
    "    print(f\"Group: {group}, Alignment: {alignment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Word Level Groups ----\n",
      "[['That'], ['is'], ['why'], ['he'], ['is'], ['a'], ['‘pretender’.'], ['He'], ['has'], ['never'], ['claimed'], ['to'], ['be'], ['a'], ['King'], ['-'], ['or'], ['a'], ['Kaiser,'], ['for'], ['that'], ['matter.'], ['He'], ['is'], ['in'], ['the'], ['same'], ['class'], ['as'], ['the'], ['Comte'], ['de'], ['Paris,'], ['who'], ['is'], ['not'], ['the'], ['King'], ['of'], ['France,'], ['but'], ['would'], ['be'], ['if'], ['the'], ['Bourbons'], ['were'], ['placed'], ['on'], ['a'], ['restored'], ['French'], ['throne.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_326434/650531704.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_emb_pt = torch.tensor(word_embs).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7166317347085701\n",
      "[[\"Let's\"], ['knock'], ['any'], [\"'EngVar'\"], ['shenanigans'], ['on'], ['the'], ['head'], ['right'], ['away,'], ['shall'], ['we?'], ['The'], ['Manual'], ['of'], ['Style,'], ['as'], ['I'], ['understand'], ['it,'], ['makes'], ['it'], ['clear'], ['that'], ['the'], [\"subject's\"], ['national'], ['ties'], ['and'], ['own'], ['language'], ['set'], ['the'], ['course.']]\n",
      "0.712827032103258\n",
      "[['Thank'], ['you'], ['for'], ['your'], ['contributions.'], ['There'], ['are'], ['some'], ['conventions'], ['that'], ['apply'], ['to'], ['articles,'], ['and'], ['medical'], ['articles'], ['in'], ['particular.'], ['Secondary'], ['sources'], ['were'], ['available'], ['for'], ['the'], ['material,'], ['and'], ['should'], ['be'], ['cited'], ['to'], ['validate'], ['the'], ['medical'], ['information'], ['from'], ['the'], ['studies.']]\n",
      "0.6841673021380966\n",
      "[['The'], ['conversion'], ['of'], ['tacit'], ['to'], ['explicit'], ['knowledge'], ['is'], ['seen'], ['in'], ['for'], ['example'], ['the'], ['bread'], ['making'], [\"machine's\"], ['case.'], ['In'], ['response'], ['to'], ['your'], ['question,'], ['culture'], ['is'], ['a'], ['broad'], ['term.'], ['I'], ['would'], ['like'], ['to'], ['narrow'], ['it'], ['down'], ['to'], ['organization'], ['culture.']]\n",
      "0.6944003032671439\n",
      "\n",
      "---- Phrase Level Groups ----\n",
      "[['That', 'is', 'why'], ['he', 'is', 'a'], ['‘pretender’.', 'He', 'has'], ['never', 'claimed', 'to'], ['be', 'a', 'King'], ['-', 'or', 'a'], ['Kaiser,', 'for', 'that'], ['matter.', 'He', 'is'], ['in', 'the', 'same'], ['class', 'as', 'the'], ['Comte', 'de', 'Paris,'], ['who', 'is', 'not'], ['the', 'King', 'of'], ['France,', 'but', 'would'], ['be', 'if', 'the'], ['Bourbons', 'were', 'placed'], ['on', 'a', 'restored'], ['French', 'throne.']]\n",
      "0.6799007488621606\n",
      "[[\"Let's\", 'knock', 'any'], [\"'EngVar'\", 'shenanigans', 'on'], ['the', 'head', 'right'], ['away,', 'shall', 'we?'], ['The', 'Manual', 'of'], ['Style,', 'as', 'I'], ['understand', 'it,', 'makes'], ['it', 'clear', 'that'], ['the', \"subject's\", 'national'], ['ties', 'and', 'own'], ['language', 'set', 'the'], ['course.']]\n",
      "0.6641653776168823\n",
      "[['Thank', 'you', 'for'], ['your', 'contributions.', 'There'], ['are', 'some', 'conventions'], ['that', 'apply', 'to'], ['articles,', 'and', 'medical'], ['articles', 'in', 'particular.'], ['Secondary', 'sources', 'were'], ['available', 'for', 'the'], ['material,', 'and', 'should'], ['be', 'cited', 'to'], ['validate', 'the', 'medical'], ['information', 'from', 'the'], ['studies.']]\n",
      "0.6234655242699844\n",
      "[['The', 'conversion', 'of'], ['tacit', 'to', 'explicit'], ['knowledge', 'is', 'seen'], ['in', 'for', 'example'], ['the', 'bread', 'making'], [\"machine's\", 'case.', 'In'], ['response', 'to', 'your'], ['question,', 'culture', 'is'], ['a', 'broad', 'term.'], ['I', 'would', 'like'], ['to', 'narrow', 'it'], ['down', 'to', 'organization'], ['culture.']]\n",
      "0.6278292353336627\n",
      "\n",
      "---- Sentence Level Groups ----\n",
      "[['That', 'is', 'why', 'he', 'is', 'a', '‘pretender’.'], ['He', 'has', 'never', 'claimed', 'to', 'be', 'a', 'King', '-', 'or', 'a', 'Kaiser,', 'for', 'that', 'matter.'], ['He', 'is', 'in', 'the', 'same', 'class', 'as', 'the', 'Comte', 'de', 'Paris,', 'who', 'is', 'not', 'the', 'King', 'of', 'France,', 'but', 'would', 'be', 'if', 'the', 'Bourbons', 'were', 'placed', 'on', 'a', 'restored', 'French', 'throne.']]\n",
      "0.6752072970072428\n",
      "[[\"Let's\", 'knock', 'any', \"'EngVar'\", 'shenanigans', 'on', 'the', 'head', 'right', 'away,', 'shall', 'we?'], ['The', 'Manual', 'of', 'Style,', 'as', 'I', 'understand', 'it,', 'makes', 'it', 'clear', 'that', 'the', \"subject's\", 'national', 'ties', 'and', 'own', 'language', 'set', 'the', 'course.']]\n",
      "0.6275225579738617\n",
      "[['Thank', 'you', 'for', 'your', 'contributions.'], ['There', 'are', 'some', 'conventions', 'that', 'apply', 'to', 'articles,', 'and', 'medical', 'articles', 'in', 'particular.'], ['Secondary', 'sources', 'were', 'available', 'for', 'the', 'material,', 'and', 'should', 'be', 'cited', 'to', 'validate', 'the', 'medical', 'information', 'from', 'the', 'studies.']]\n",
      "0.6261236866315206\n",
      "[['The', 'conversion', 'of', 'tacit', 'to', 'explicit', 'knowledge', 'is', 'seen', 'in', 'for', 'example', 'the', 'bread', 'making', \"machine's\", 'case.'], ['In', 'response', 'to', 'your', 'question,', 'culture', 'is', 'a', 'broad', 'term.'], ['I', 'would', 'like', 'to', 'narrow', 'it', 'down', 'to', 'organization', 'culture.']]\n",
      "0.6264440417289734\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader: \n",
    "    word_lists = batch['word_list']\n",
    "    word_lists = list(map(list, zip(*word_lists)))\n",
    "    processed_word_lists = []\n",
    "    for word_list in word_lists:\n",
    "        processed_word_lists.append([word for word in word_list if word != ''])\n",
    "    print(\"---- Word Level Groups ----\")\n",
    "    # word_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        word_groups = []\n",
    "        for word in word_list:\n",
    "            word_groups.append([word])\n",
    "        print(word_groups)\n",
    "        alignments = metric.calculate_group_alignment(word_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    print(\"\\n---- Phrase Level Groups ----\")\n",
    "    # phrase_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        phrase_groups = []\n",
    "        #each group is 3 consecutive words\n",
    "        for i in range(0, len(word_list), 3):\n",
    "            phrase_groups.append(word_list[i:i+3])\n",
    "        print(phrase_groups)\n",
    "        alignments = metric.calculate_group_alignment(phrase_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    print(\"\\n---- Sentence Level Groups ----\")\n",
    "    # sentence_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        sentence_groups = []\n",
    "\n",
    "        #reconstruct sentences from word list\n",
    "        sentence = \"\"\n",
    "        for word in word_list:\n",
    "            sentence += word + \" \"\n",
    "            if word[-1] == \".\" or word[-1] == \"!\" or word[-1] == \"?\":\n",
    "                sentence_groups.append(sentence.split())\n",
    "                sentence = \"\"\n",
    "        if(len(sentence) > 0):\n",
    "            sentence_groups.append(sentence.split())\n",
    "\n",
    "        print(sentence_groups)\n",
    "        alignments = metric.calculate_group_alignment(sentence_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtop",
   "language": "python",
   "name": "vtop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
