{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment line below to install exlib\n",
    "# !pip install exlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import sentence_transformers\n",
    "\n",
    "import exlib\n",
    "from exlib.utils.politeness_helper import load_lexica\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_REPO = \"BrachioLab/multilingual_politeness\"\n",
    "MODEL_REPO = \"BrachioLab/xlm-roberta-politeness\"\n",
    "TOKENIZER_REPO = \"xlm-roberta-base\"\n",
    "\n",
    "def load_data():\n",
    "    hf_dataset = load_dataset(DATASET_REPO)\n",
    "    return hf_dataset\n",
    "\n",
    "def load_model():\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(MODEL_REPO)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "class PolitenessDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split, language=\"english\"):\n",
    "        dataset = load_dataset(DATASET_REPO)[split]\n",
    "        dataset = dataset.filter(lambda x: x[\"language\"] == language)\n",
    "        dataset = dataset.rename_column(\"politeness\", \"label\")\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(TOKENIZER_REPO)\n",
    "        self.max_len = max([len(text.split()) for text in dataset['Utterance']])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[\"Utterance\"][idx]\n",
    "        label = self.dataset[\"label\"][idx]\n",
    "        encoding = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "        word_list = text.split()\n",
    "        for i in range(len(word_list), self.max_len):\n",
    "            word_list.append('')\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"label\": torch.tensor(label),\n",
    "            'word_list': word_list\n",
    "        }\n",
    "\n",
    "class PolitenessClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolitenessClassifier, self).__init__()\n",
    "        self.model = load_model()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        logits = outputs.logits\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample inference on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6860e6b833e24d41b900e9208d2bf58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub-0.23.2-py3.8.egg/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  0% 0/1140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: That is why he is a ‘pretender’. He has never claimed to be a King - or a Kaiser, for that matter. He is in the same class as the Comte de Paris, who is not the King of France, but would be if the Bourbons were placed on a restored French throne.\n",
      "Politeness: -0.11096354573965073\n",
      "\n",
      "Text: Let's knock any 'EngVar' shenanigans on the head right away, shall we? The Manual of Style, as I understand it, makes it clear that the subject's national ties and own language set the course.\n",
      "Politeness: -0.4824686050415039\n",
      "\n",
      "Text: Thank you for your contributions. There are some conventions that apply to articles, and medical articles in particular. Secondary sources were available for the material, and should be cited to validate the medical information from the studies.\n",
      "Politeness: 1.592445731163025\n",
      "\n",
      "Text: The conversion of tacit to explicit knowledge is seen in for example the bread making machine's case. In response to your question, culture is a broad term. I would like to narrow it down to organization culture.\n",
      "Politeness: 0.7522320747375488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = PolitenessDataset(\"train\")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "model = PolitenessClassifier()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(dataloader): \n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    utterances = [dataset.tokenizer.decode(input_id, skip_special_tokens=True) for input_id in input_ids]\n",
    "    for utterance, label in zip(utterances, output):\n",
    "        print(\"Text: {}\\nPoliteness: {}\\n\".format(utterance, label.item()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Alignment Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(nn.Module): \n",
    "    def __init__(self, model_name:str=\"distiluse-base-multilingual-cased\"): \n",
    "        super(Metric, self).__init__()\n",
    "        self.model = sentence_transformers.SentenceTransformer(model_name)\n",
    "        self.centroids = self.get_centroids()\n",
    "    \n",
    "    def get_centroids(self):\n",
    "        # read lexica files\n",
    "        languages = [\"english\", \"spanish\", \"chinese\", \"japanese\"]\n",
    "        lexica = {}\n",
    "        for l in languages:\n",
    "            lexica[l] = load_lexica(l)\n",
    "\n",
    "        # create centroids\n",
    "        all_centroids = {}        \n",
    "        for l in languages:\n",
    "            categories = lexica[l][\"CATEGORY\"].unique()\n",
    "            centroids = {}\n",
    "            for c in categories:\n",
    "                words = lexica[l][lexica[l][\"CATEGORY\"] == c][\"word\"].tolist()\n",
    "                embeddings = self.model.encode(words)\n",
    "                centroid = np.mean(embeddings, axis=0)\n",
    "                centroids[c] = centroid\n",
    "            assert len(categories) == len(centroids.keys())\n",
    "            all_centroids[l] = centroids\n",
    "            print(f\"Centroids for {l} created.\")\n",
    "        return all_centroids\n",
    "\n",
    "    # input: list of words\n",
    "    def calculate_single_group_alignment(self, group:list, language:str=\"english\"):\n",
    "        #find max avg cos sim between word embeddings and centroids\n",
    "        category_similarities = {}\n",
    "        centroids = self.centroids[language]\n",
    "        for category, centroid_emb in centroids.items():\n",
    "            #calculate cosine similarity\n",
    "            cos_sim = []\n",
    "            for word in group:\n",
    "                word_emb = self.model.encode(word)\n",
    "                cos_sim.append(np.dot(word_emb, centroid_emb) / (np.linalg.norm(word_emb) * np.linalg.norm(centroid_emb)))\n",
    "            avg_cos_sim = np.mean(cos_sim)\n",
    "            category_similarities[category] = avg_cos_sim\n",
    "        #return highest similarity score\n",
    "        return max(category_similarities.values())\n",
    "\n",
    "    # input: list of words\n",
    "    def calculate_single_group_alignment(self, group:list, language:str=\"english\"):\n",
    "        #find max avg cos sim between word embeddings and centroids\n",
    "        category_similarities = {}\n",
    "        centroids = self.centroids[language]\n",
    "        word_embs = []\n",
    "        for word in group:\n",
    "            word_emb = self.model.encode(word)\n",
    "            word_embs.append(torch.tensor(word_emb))\n",
    "\n",
    "        # word_embs = self.model.encode(group)\n",
    "        word_embs = torch.stack(word_embs).to(device)\n",
    "        word_emb_pt = torch.tensor(word_embs).to(device)\n",
    "        centroid_embs = list(centroids.values())\n",
    "        centroid_emb_pt = torch.tensor(centroid_embs).to(device)\n",
    "\n",
    "        # Compute the norms for each batch\n",
    "        norm_word = torch.norm(word_emb_pt, dim=1, keepdim=True)  # Shape becomes (n, 1)\n",
    "        norm_centroid = torch.norm(centroid_emb_pt, dim=1, keepdim=True)  # Shape becomes (m, 1)\n",
    "\n",
    "        # Compute the dot products\n",
    "        # Transpose centroid_emb_pt to make it (d, m) for matrix multiplication\n",
    "        dot_product = torch.mm(word_emb_pt, centroid_emb_pt.T)  # Resulting shape is (n, m)\n",
    "\n",
    "        # Compute the outer product of the norms\n",
    "        norms_product = torch.mm(norm_word, norm_centroid.T)  # Resulting shape is (n, m)\n",
    "\n",
    "        # Calculate the cosine similarity matrix\n",
    "        cosine_similarity = dot_product / norms_product\n",
    "\n",
    "        group_alignment = cosine_similarity.mean(0).max().item()\n",
    "        return group_alignment\n",
    "\n",
    "    def calculate_group_alignment(self, groups:list, language:str=\"english\"):\n",
    "        group_alignments = []\n",
    "        for group in groups:\n",
    "            group_alignments.append(self.calculate_single_group_alignment(group, language))\n",
    "        return group_alignments\n",
    "    \n",
    "    def forward(self, group_masks:list, original_data:PolitenessDataset, language=\"english\"):\n",
    "        #create groups\n",
    "        groups = []\n",
    "        for i in range(len(group_masks)):\n",
    "            word_list_ex = original_data[i]['word_list']\n",
    "            mask = group_masks[i]\n",
    "            print(word_list_ex, mask)\n",
    "            group = [word_list_ex[j] for j in range(len(mask)) if mask[j] == 1]\n",
    "            groups.append(group)\n",
    "        return np.mean(self.calculate_group_alignment(groups, language))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Expert Alignment Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub-0.23.2-py3.8.egg/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/runai-home/.local/lib/python3.10/site-packages/exlib/utils/politeness_lexica/english_politelex.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43mMetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m sample_groups \u001b[38;5;241m=\u001b[39m [ [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoodbye\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      3\u001b[0m                 [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlaptop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphone\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      4\u001b[0m                 [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midiot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstupid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdumb\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      5\u001b[0m                 [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthank you\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrateful\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthanks\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      6\u001b[0m alignments \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcalculate_group_alignment(sample_groups)\n",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m, in \u001b[0;36mMetric.__init__\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(Metric, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer(model_name)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcentroids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_centroids\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mMetric.get_centroids\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m lexica \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m languages:\n\u001b[0;32m---> 12\u001b[0m     lexica[l] \u001b[38;5;241m=\u001b[39m \u001b[43mload_lexica\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# create centroids\u001b[39;00m\n\u001b[1;32m     15\u001b[0m all_centroids \u001b[38;5;241m=\u001b[39m {}        \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/exlib/utils/politeness_helper.py:9\u001b[0m, in \u001b[0;36mload_lexica\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lexica\u001b[39m(language):\n\u001b[1;32m      8\u001b[0m     parent_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoliteness_lexica/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_politelex.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/runai-home/.local/lib/python3.10/site-packages/exlib/utils/politeness_lexica/english_politelex.csv'"
     ]
    }
   ],
   "source": [
    "metric = Metric()\n",
    "sample_groups = [ [\"hello\", \"goodbye\", \"please\"], \n",
    "                [\"computer\", \"laptop\", \"phone\"], \n",
    "                [\"idiot\", \"stupid\", \"dumb\"], \n",
    "                [\"thank you\", \"grateful\", \"thanks\"]]\n",
    "alignments = metric.calculate_group_alignment(sample_groups)\n",
    "for group, alignment in zip(sample_groups, alignments):\n",
    "    print(f\"Group: {group}, Alignment: {alignment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Word Level Groups ----\n",
      "[['That'], ['is'], ['why'], ['he'], ['is'], ['a'], ['‘pretender’.'], ['He'], ['has'], ['never'], ['claimed'], ['to'], ['be'], ['a'], ['King'], ['-'], ['or'], ['a'], ['Kaiser,'], ['for'], ['that'], ['matter.'], ['He'], ['is'], ['in'], ['the'], ['same'], ['class'], ['as'], ['the'], ['Comte'], ['de'], ['Paris,'], ['who'], ['is'], ['not'], ['the'], ['King'], ['of'], ['France,'], ['but'], ['would'], ['be'], ['if'], ['the'], ['Bourbons'], ['were'], ['placed'], ['on'], ['a'], ['restored'], ['French'], ['throne.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_326434/650531704.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_emb_pt = torch.tensor(word_embs).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7166317347085701\n",
      "[[\"Let's\"], ['knock'], ['any'], [\"'EngVar'\"], ['shenanigans'], ['on'], ['the'], ['head'], ['right'], ['away,'], ['shall'], ['we?'], ['The'], ['Manual'], ['of'], ['Style,'], ['as'], ['I'], ['understand'], ['it,'], ['makes'], ['it'], ['clear'], ['that'], ['the'], [\"subject's\"], ['national'], ['ties'], ['and'], ['own'], ['language'], ['set'], ['the'], ['course.']]\n",
      "0.712827032103258\n",
      "[['Thank'], ['you'], ['for'], ['your'], ['contributions.'], ['There'], ['are'], ['some'], ['conventions'], ['that'], ['apply'], ['to'], ['articles,'], ['and'], ['medical'], ['articles'], ['in'], ['particular.'], ['Secondary'], ['sources'], ['were'], ['available'], ['for'], ['the'], ['material,'], ['and'], ['should'], ['be'], ['cited'], ['to'], ['validate'], ['the'], ['medical'], ['information'], ['from'], ['the'], ['studies.']]\n",
      "0.6841673021380966\n",
      "[['The'], ['conversion'], ['of'], ['tacit'], ['to'], ['explicit'], ['knowledge'], ['is'], ['seen'], ['in'], ['for'], ['example'], ['the'], ['bread'], ['making'], [\"machine's\"], ['case.'], ['In'], ['response'], ['to'], ['your'], ['question,'], ['culture'], ['is'], ['a'], ['broad'], ['term.'], ['I'], ['would'], ['like'], ['to'], ['narrow'], ['it'], ['down'], ['to'], ['organization'], ['culture.']]\n",
      "0.6944003032671439\n",
      "\n",
      "---- Phrase Level Groups ----\n",
      "[['That', 'is', 'why'], ['he', 'is', 'a'], ['‘pretender’.', 'He', 'has'], ['never', 'claimed', 'to'], ['be', 'a', 'King'], ['-', 'or', 'a'], ['Kaiser,', 'for', 'that'], ['matter.', 'He', 'is'], ['in', 'the', 'same'], ['class', 'as', 'the'], ['Comte', 'de', 'Paris,'], ['who', 'is', 'not'], ['the', 'King', 'of'], ['France,', 'but', 'would'], ['be', 'if', 'the'], ['Bourbons', 'were', 'placed'], ['on', 'a', 'restored'], ['French', 'throne.']]\n",
      "0.6799007488621606\n",
      "[[\"Let's\", 'knock', 'any'], [\"'EngVar'\", 'shenanigans', 'on'], ['the', 'head', 'right'], ['away,', 'shall', 'we?'], ['The', 'Manual', 'of'], ['Style,', 'as', 'I'], ['understand', 'it,', 'makes'], ['it', 'clear', 'that'], ['the', \"subject's\", 'national'], ['ties', 'and', 'own'], ['language', 'set', 'the'], ['course.']]\n",
      "0.6641653776168823\n",
      "[['Thank', 'you', 'for'], ['your', 'contributions.', 'There'], ['are', 'some', 'conventions'], ['that', 'apply', 'to'], ['articles,', 'and', 'medical'], ['articles', 'in', 'particular.'], ['Secondary', 'sources', 'were'], ['available', 'for', 'the'], ['material,', 'and', 'should'], ['be', 'cited', 'to'], ['validate', 'the', 'medical'], ['information', 'from', 'the'], ['studies.']]\n",
      "0.6234655242699844\n",
      "[['The', 'conversion', 'of'], ['tacit', 'to', 'explicit'], ['knowledge', 'is', 'seen'], ['in', 'for', 'example'], ['the', 'bread', 'making'], [\"machine's\", 'case.', 'In'], ['response', 'to', 'your'], ['question,', 'culture', 'is'], ['a', 'broad', 'term.'], ['I', 'would', 'like'], ['to', 'narrow', 'it'], ['down', 'to', 'organization'], ['culture.']]\n",
      "0.6278292353336627\n",
      "\n",
      "---- Sentence Level Groups ----\n",
      "[['That', 'is', 'why', 'he', 'is', 'a', '‘pretender’.'], ['He', 'has', 'never', 'claimed', 'to', 'be', 'a', 'King', '-', 'or', 'a', 'Kaiser,', 'for', 'that', 'matter.'], ['He', 'is', 'in', 'the', 'same', 'class', 'as', 'the', 'Comte', 'de', 'Paris,', 'who', 'is', 'not', 'the', 'King', 'of', 'France,', 'but', 'would', 'be', 'if', 'the', 'Bourbons', 'were', 'placed', 'on', 'a', 'restored', 'French', 'throne.']]\n",
      "0.6752072970072428\n",
      "[[\"Let's\", 'knock', 'any', \"'EngVar'\", 'shenanigans', 'on', 'the', 'head', 'right', 'away,', 'shall', 'we?'], ['The', 'Manual', 'of', 'Style,', 'as', 'I', 'understand', 'it,', 'makes', 'it', 'clear', 'that', 'the', \"subject's\", 'national', 'ties', 'and', 'own', 'language', 'set', 'the', 'course.']]\n",
      "0.6275225579738617\n",
      "[['Thank', 'you', 'for', 'your', 'contributions.'], ['There', 'are', 'some', 'conventions', 'that', 'apply', 'to', 'articles,', 'and', 'medical', 'articles', 'in', 'particular.'], ['Secondary', 'sources', 'were', 'available', 'for', 'the', 'material,', 'and', 'should', 'be', 'cited', 'to', 'validate', 'the', 'medical', 'information', 'from', 'the', 'studies.']]\n",
      "0.6261236866315206\n",
      "[['The', 'conversion', 'of', 'tacit', 'to', 'explicit', 'knowledge', 'is', 'seen', 'in', 'for', 'example', 'the', 'bread', 'making', \"machine's\", 'case.'], ['In', 'response', 'to', 'your', 'question,', 'culture', 'is', 'a', 'broad', 'term.'], ['I', 'would', 'like', 'to', 'narrow', 'it', 'down', 'to', 'organization', 'culture.']]\n",
      "0.6264440417289734\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader: \n",
    "    word_lists = batch['word_list']\n",
    "    word_lists = list(map(list, zip(*word_lists)))\n",
    "    processed_word_lists = []\n",
    "    for word_list in word_lists:\n",
    "        processed_word_lists.append([word for word in word_list if word != ''])\n",
    "    print(\"---- Word Level Groups ----\")\n",
    "    # word_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        word_groups = []\n",
    "        for word in word_list:\n",
    "            word_groups.append([word])\n",
    "        print(word_groups)\n",
    "        alignments = metric.calculate_group_alignment(word_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    print(\"\\n---- Phrase Level Groups ----\")\n",
    "    # phrase_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        phrase_groups = []\n",
    "        #each group is 3 consecutive words\n",
    "        for i in range(0, len(word_list), 3):\n",
    "            phrase_groups.append(word_list[i:i+3])\n",
    "        print(phrase_groups)\n",
    "        alignments = metric.calculate_group_alignment(phrase_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    print(\"\\n---- Sentence Level Groups ----\")\n",
    "    # sentence_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        sentence_groups = []\n",
    "\n",
    "        #reconstruct sentences from word list\n",
    "        sentence = \"\"\n",
    "        for word in word_list:\n",
    "            sentence += word + \" \"\n",
    "            if word[-1] == \".\" or word[-1] == \"!\" or word[-1] == \"?\":\n",
    "                sentence_groups.append(sentence.split())\n",
    "                sentence = \"\"\n",
    "        if(len(sentence) > 0):\n",
    "            sentence_groups.append(sentence.split())\n",
    "\n",
    "        print(sentence_groups)\n",
    "        alignments = metric.calculate_group_alignment(sentence_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
