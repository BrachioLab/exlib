{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIX - Multilingual Politeness Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIX is built using the `exlib` library, which we load using a local version for now. You can uncomment the `!pip install exlib` line and comment out the `import sys; sys.path.insert(0, \"../../src\")` line if you do not have a local version you are testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment line below to install exlib\n",
    "# !pip install exlib\n",
    "import sys; sys.path.insert(0, \"../../src\")\n",
    "import exlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import sentence_transformers\n",
    "\n",
    "from exlib.datasets.multilingual_politeness import PolitenessDataset, PolitenessClassifier, PolitenessFixScore, get_politeness_scores\n",
    "from exlib.datasets.politeness_helper import load_lexica\n",
    "\n",
    "from exlib.features.text import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets and pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample inference on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at BrachioLab/xlm-roberta-politeness and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "dataset = PolitenessDataset(\"test\")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "model = PolitenessClassifier()\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/143 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The intro mentions the ISO 8601 international standard adopted in most western countries. What does this even mean? Who are we suggesting has done the adoption?\n",
      "Politeness: 0.21642041206359863\n",
      "\n",
      "Text: I'm a user on PrettyCure.org, and somebody on the site said they are making a fourth season of PreCure. It's a rumuor, but is it true? That person said it's more like Tokyo Mew Mew, a group of girls.\n",
      "Politeness: 0.21494880318641663\n",
      "\n",
      "Text: Hello fellow Wikipedians, I have just added archive links to on Essen. Please take a moment to review my edit. If necessary, add after the link to keep me from modifying it.\n",
      "Politeness: 0.12163643538951874\n",
      "\n",
      "Text: I saw the template citing this issue and since there was no section here discussing it I've decided to start one. I'm a Canadian and most of our television programs are also aired in the US so my knowledge of what's on TV outside of North America is limited. So I'm not sure of how much help I can be, but I do have some ideas on how to improve this section and I'm open to feedback.\n",
      "Politeness: 0.09295740723609924\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(dataloader): \n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    utterances = [dataset.tokenizer.decode(input_id, skip_special_tokens=True) for input_id in input_ids]\n",
    "    for utterance, label in zip(utterances, output):\n",
    "        print(\"Text: {}\\nPoliteness: {}\\n\".format(utterance, label.item()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at BrachioLab/xlm-roberta-politeness and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- word Level Groups ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 143/143 [01:11<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- phrase Level Groups ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 143/143 [01:50<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- sentence Level Groups ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 143/143 [01:33<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "all_baselines_scores = get_politeness_scores(baselines = ['word', 'phrase', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE word mean score: 0.6839182740178517\n",
      "BASELINE phrase mean score: 0.6350535143089757\n",
      "BASELINE sentence mean score: 0.6108726882043238\n"
     ]
    }
   ],
   "source": [
    "for name in all_baselines_scores:\n",
    "    metric = torch.tensor(all_baselines_scores[name])\n",
    "    mean_metric = metric.nanmean()\n",
    "    print(f'BASELINE {name} mean score: {mean_metric}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTopic (Clustering)\n",
    "\n",
    "create topics from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, save all the utterances\n",
    "\n",
    "dataset = PolitenessDataset(\"test\")\n",
    "utterances = [' '.join(dataset[i]['word_list']) for i in range(len(dataset))]\n",
    "# torch.save(utterances, '../../fix/utterances/multilingual_politeness_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (0.16.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from bertopic) (1.26.4)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from bertopic) (0.8.38.post1)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from bertopic) (0.5.6)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from bertopic) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from bertopic) (1.5.1)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from bertopic) (4.66.4)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from bertopic) (3.1.1)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from bertopic) (5.24.1)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from hdbscan>=0.8.29->bertopic) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2024.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from plotly>=4.7.0->bertopic) (9.0.0)\n",
      "Requirement already satisfied: packaging in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from plotly>=4.7.0->bertopic) (24.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.42.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.23.4)\n",
      "Requirement already satisfied: Pillow in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from sentence-transformers>=0.4.1->bertopic) (10.4.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
      "Requirement already satisfied: filelock in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from huggingface-hub>=0.19.3->sentence-transformers>=0.4.1->bertopic) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from huggingface-hub>=0.19.3->sentence-transformers>=0.4.1->bertopic) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from huggingface-hub>=0.19.3->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from huggingface-hub>=0.19.3->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from huggingface-hub>=0.19.3->sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: sympy in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers>=0.4.1->bertopic) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers>=0.4.1->bertopic) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers>=0.4.1->bertopic) (0.4.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers>=0.4.1->bertopic) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers>=0.4.1->bertopic) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/antonxue/lib/miniconda3/envs/exlib/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# then, install bertopic + use them on the utterances\n",
    "\n",
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at BrachioLab/xlm-roberta-politeness and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- clustering Level Groups ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 143/143 [00:14<00:00,  9.55it/s]\n"
     ]
    }
   ],
   "source": [
    "all_baselines_scores = get_politeness_scores(baselines = ['clustering'])\n",
    "# make sure utterances_path is set to where utteraces is saved in your directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE clustering mean score: 0.6679689280296627\n"
     ]
    }
   ],
   "source": [
    "for name, score in all_baselines_scores.items():\n",
    "    print(f'BASELINE {name} mean score: {score.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
