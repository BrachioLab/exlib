{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import TextClassificationPipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "from transformers import Pipeline\n",
    "from torch import Tensor \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "import sentence_transformers\n",
    "\n",
    "sys.path.append(os.path.abspath('../src/exlib/utils'))\n",
    "from projection_helper import project_points_onto_axes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_REPO = \"go_emotions\"\n",
    "MODEL_REPO = \"shreyahavaldar/roberta-base-go_emotions\"\n",
    "TOKENIZER_REPO = \"roberta-base\"\n",
    "\n",
    "def load_data():\n",
    "    hf_dataset = load_dataset(DATASET_REPO)\n",
    "    return hf_dataset\n",
    "\n",
    "def load_model():\n",
    "    model = AutoModel.from_pretrained(MODEL_REPO)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "#go emotions dataset\n",
    "class EmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        dataset = load_dataset(DATASET_REPO)[split]        \n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_REPO)\n",
    "        self.max_len = max([len(text.split()) for text in dataset['text']])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        label = self.dataset[idx]['labels'][0]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        word_list = text.split()\n",
    "        for i in range(len(word_list), self.max_len):\n",
    "            word_list.append('')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label),\n",
    "            'word_list': word_list\n",
    "        }\n",
    "\n",
    "#classifier for go emotions dataset\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(MODEL_REPO)\n",
    "        self.classifier = nn.Linear(768, 28)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        cls_token = last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_token)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample inference on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at shreyahavaldar/roberta-base-go_emotions and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/10853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My favourite food is anything I didn't have to cook myself.\n",
      "Emotion: 21\n",
      "\n",
      "Text: Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead\n",
      "Emotion: 12\n",
      "\n",
      "Text: WHY THE FUCK IS BAYLESS ISOING\n",
      "Emotion: 12\n",
      "\n",
      "Text: To make her feel threatened\n",
      "Emotion: 15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = EmotionDataset(\"train\")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "model = EmotionClassifier()\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(dataloader): \n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    utterances = [dataset.tokenizer.decode(input_id, skip_special_tokens=True) for input_id in input_ids]\n",
    "    for utterance, label in zip(utterances, output):\n",
    "        print(\"Text: {}\\nEmotion: {}\\n\".format(utterance, label.argmax()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Alignment Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(nn.Module): \n",
    "    def __init__(self, model_name:str=\"all-mpnet-base-v2\"): \n",
    "        super(Metric, self).__init__()\n",
    "        self.model = sentence_transformers.SentenceTransformer(model_name)\n",
    "        points = self.define_circumplex()\n",
    "        self.x1 = points[0]\n",
    "        self.x2 = points[1]\n",
    "        self.y1 = points[3]\n",
    "        self.y2 = points[2]\n",
    "\n",
    "    def define_circumplex(self):\n",
    "        emotions = pd.read_csv(\"../src/exlib/utils/russell_emotions.csv\")\n",
    "        axis_labels = [\"NV\", \"PV\", \"HA\", \"LA\"]\n",
    "        axis_points = []\n",
    "        for label in axis_labels:\n",
    "            emotion_words = emotions[emotions[\"label\"] == label][\"emotion\"].values\n",
    "            emotion_embeddings = self.model.encode(emotion_words)\n",
    "            axis_points.append(np.mean(emotion_embeddings, axis=0))\n",
    "        return axis_points\n",
    "    \n",
    "    def distance_from_circumplex(self, embeddings):\n",
    "        projection = project_points_onto_axes(embeddings, self.x1, self.x2, self.y1, self.y2)\n",
    "        x_projections = projection[0]\n",
    "        y_projections = projection[1]\n",
    "        distances = []\n",
    "        for x, y in zip(x_projections, y_projections):\n",
    "            distances.append(np.abs(np.sqrt(x**2 + y**2)-1))\n",
    "        return 1/np.mean(distances)\n",
    "\n",
    "    def mean_pairwise_dist(self, embeddings):\n",
    "        projection = project_points_onto_axes(embeddings, self.x1, self.x2, self.y1, self.y2)\n",
    "        distances = []\n",
    "        x_coords = projection[0]\n",
    "        y_coords = projection[1]\n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(i+1, len(embeddings)):\n",
    "                x_dist = x_coords[i] - x_coords[j]\n",
    "                y_dist = y_coords[i] - y_coords[j]\n",
    "                distances.append(np.sqrt(x_dist**2 + y_dist**2))\n",
    "        return 1/np.mean(distances)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    # input: list of words\n",
    "    def calculate_group_alignment(self, groups:list, language:str=\"english\"):\n",
    "        alignments = []\n",
    "        for group in groups:\n",
    "            embeddings = self.model.encode(group)\n",
    "            circumplex_dist = self.distance_from_circumplex(embeddings)\n",
    "            if(len(embeddings) == 1): \n",
    "                alignments.append(circumplex_dist)\n",
    "            else:\n",
    "                mean_dist = self.mean_pairwise_dist(embeddings)\n",
    "                combined_dist = circumplex_dist*mean_dist\n",
    "                alignments.append(combined_dist)\n",
    "        return alignments\n",
    "        \n",
    "    def forward(self, zp, x=None, y=None, z=None, reduce=True, **kwargs): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Expert Alignment Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: ['hooray!', 'yay!', 'surprise!'], Alignment: 4.486955276930485\n",
      "Group: ['happy', 'excited'], Alignment: 8.559298682657115\n",
      "Group: ['beautiful', 'ugly'], Alignment: 2.1811173063494373\n"
     ]
    }
   ],
   "source": [
    "metric = Metric()\n",
    "sample_groups = [[\"hooray!\", \"yay!\", \"surprise!\"], \n",
    "                [\"happy\", \"excited\"],\n",
    "                [\"beautiful\", \"ugly\"]]\n",
    "\n",
    "alignments = metric.calculate_group_alignment(sample_groups)\n",
    "\n",
    "for group, alignment in zip(sample_groups, alignments):\n",
    "    print(f\"Group: {group}, Alignment: {alignment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Word Level Groups ----\n",
      "[['My'], ['favourite'], ['food'], ['is'], ['anything'], ['I'], [\"didn't\"], ['have'], ['to'], ['cook'], ['myself.']]\n",
      "1.2176552793157727\n",
      "[['Now'], ['if'], ['he'], ['does'], ['off'], ['himself,'], ['everyone'], ['will'], ['think'], ['hes'], ['having'], ['a'], ['laugh'], ['screwing'], ['with'], ['people'], ['instead'], ['of'], ['actually'], ['dead']]\n",
      "1.2696060271921354\n",
      "[['WHY'], ['THE'], ['FUCK'], ['IS'], ['BAYLESS'], ['ISOING']]\n",
      "1.2578318794807875\n",
      "[['To'], ['make'], ['her'], ['feel'], ['threatened']]\n",
      "1.8772896491302233\n",
      "\n",
      "---- Phrase Level Groups ----\n",
      "[['My', 'favourite', 'food'], ['is', 'anything', 'I'], [\"didn't\", 'have', 'to'], ['cook', 'myself.']]\n",
      "5.190700584699706\n",
      "[['Now', 'if', 'he'], ['does', 'off', 'himself,'], ['everyone', 'will', 'think'], ['hes', 'having', 'a'], ['laugh', 'screwing', 'with'], ['people', 'instead', 'of'], ['actually', 'dead']]\n",
      "5.836896290912729\n",
      "[['WHY', 'THE', 'FUCK'], ['IS', 'BAYLESS', 'ISOING']]\n",
      "4.158414830104384\n",
      "[['To', 'make', 'her'], ['feel', 'threatened']]\n",
      "5.475611106249764\n",
      "\n",
      "---- Sentence Level Groups ----\n",
      "[['My', 'favourite', 'food', 'is', 'anything', 'I', \"didn't\", 'have', 'to', 'cook', 'myself.']]\n",
      "4.953790711091848\n",
      "[['Now', 'if', 'he', 'does', 'off', 'himself,', 'everyone', 'will', 'think', 'hes', 'having', 'a', 'laugh', 'screwing', 'with', 'people', 'instead', 'of', 'actually', 'dead']]\n",
      "4.483586311320412\n",
      "[['WHY', 'THE', 'FUCK', 'IS', 'BAYLESS', 'ISOING']]\n",
      "4.559811363834015\n",
      "[['To', 'make', 'her', 'feel', 'threatened']]\n",
      "3.0411445874427585\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader: \n",
    "    word_lists = batch['word_list']\n",
    "    word_lists = list(map(list, zip(*word_lists)))\n",
    "    processed_word_lists = []\n",
    "    for word_list in word_lists:\n",
    "        processed_word_lists.append([word for word in word_list if word != ''])\n",
    "    print(\"---- Word Level Groups ----\")\n",
    "    word_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        word_groups = []\n",
    "        for word in word_list:\n",
    "            word_groups.append([word])\n",
    "        print(word_groups)\n",
    "        alignments = metric.calculate_group_alignment(word_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    print(\"\\n---- Phrase Level Groups ----\")\n",
    "    phrase_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        phrase_groups = []\n",
    "        #each group is 3 consecutive words\n",
    "        for i in range(0, len(word_list), 3):\n",
    "            phrase_groups.append(word_list[i:i+3])\n",
    "        print(phrase_groups)\n",
    "        alignments = metric.calculate_group_alignment(phrase_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    print(\"\\n---- Sentence Level Groups ----\")\n",
    "    sentence_alignments = []\n",
    "    for word_list in processed_word_lists:\n",
    "        sentence_groups = []\n",
    "\n",
    "        #reconstruct sentences from word list\n",
    "        sentence = \"\"\n",
    "        for word in word_list:\n",
    "            sentence += word + \" \"\n",
    "            if word[-1] == \".\" or word[-1] == \"!\" or word[-1] == \"?\":\n",
    "                sentence_groups.append(sentence.split())\n",
    "                sentence = \"\"\n",
    "        if(len(sentence) > 0):\n",
    "            sentence_groups.append(sentence.split())\n",
    "\n",
    "        print(sentence_groups)\n",
    "        alignments = metric.calculate_group_alignment(sentence_groups)\n",
    "        print(np.mean(alignments))\n",
    "\n",
    "    break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtop",
   "language": "python",
   "name": "vtop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
